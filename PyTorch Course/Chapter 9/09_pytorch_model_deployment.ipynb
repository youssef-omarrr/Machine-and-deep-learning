{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790a8e19",
   "metadata": {},
   "source": [
    "# 09. PyTorch Model Deployment\n",
    "\n",
    "Welcome to Milestone Project 3: PyTorch Model Deployment!\n",
    "\n",
    "We've come a long way with our FoodVision Mini project.\n",
    "\n",
    "But so far our PyTorch models have only been accessible to us.\n",
    "\n",
    "How about we bring FoodVision Mini to life and make it publically accessible?\n",
    "\n",
    "In other words, **we're going to deploy our FoodVision Mini model to the internet as a usable app!**\n",
    "\n",
    "*Trying out the [deployed version of FoodVision Mini](https://huggingface.co/spaces/mrdbourke/foodvision_mini) (what we're going to build) on my lunch. The model got it right too 🍣!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2581a62",
   "metadata": {},
   "source": [
    "## Different types of machine learning model deployment\n",
    "\n",
    "Whole books could be written on the different types of machine learning model deployment (and many good ones are listed in [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering)).\n",
    "\n",
    "And the field is still developing in terms of best practices.\n",
    "\n",
    "But I like to start with the question:\n",
    "\n",
    "> \"What is the most ideal scenario for my machine learning model to be used?\"\n",
    "\n",
    "And then work backwards from there.\n",
    "\n",
    "Of course, you may not know this ahead of time. But you're smart enough to imagine such things.\n",
    "\n",
    "In the case of FoodVision Mini, our ideal scenario might be:\n",
    "\n",
    "* Someone takes a photo on a mobile device (through an app or web broswer).\n",
    "* The prediction comes back fast.\n",
    "\n",
    "Easy.\n",
    "\n",
    "So we've got two main criteria:\n",
    "\n",
    "1. The model should work on a mobile device (this means there will be some compute constraints). \n",
    "2. The model should make predictions *fast* (because a slow app is a boring app).\n",
    "\n",
    "And of course, depending on your use case, your requirements may vary.\n",
    "\n",
    "You may notice the above two points break down into another two questions:\n",
    "\n",
    "1. **Where's it going to go?** - As in, where is it going to be stored?\n",
    "2. **How's it going to function?** - As in, does it return predictions immediately? Or do they come later?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-deployment-questions-to-ask.png\" alt=\"some questions to ask when starting to deploy machine learning models, what's the model ideal use case, then work backwards and ask where's my model going to go and how's my model going to function\" width=900/>\n",
    "\n",
    "*When starting to deploy machine learning models, it's helpful to start by asking what's the most ideal use case and then work backwards from there, asking where the model's going to go and then how it's going to function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30177c09",
   "metadata": {},
   "source": [
    "### Where's it going to go?\n",
    "\n",
    "When you deploy your machine learning model, where does it live?\n",
    "\n",
    "The main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isn't the *actual* device someone/something calls the model from). \n",
    "\n",
    "Both have their pros and cons.\n",
    "\n",
    "| **Deployment location** | **Pros** | **Cons** | \n",
    "| ----- | ----- | ----- |\n",
    "| **On-device (edge/in the browser)** | Can be very fast (since no data leaves the device) | Limited compute power (larger models take longer to run) | \n",
    "| | Privacy preserving (again no data has to leave the device) | Limited storage space (smaller model size required) | \n",
    "| | No internet connection required (sometimes) | Device-specific skills often required | \n",
    "| | | | \n",
    "| **On cloud** | Near unlimited compute power (can scale up when needed) | Costs can get out of hand (if proper scaling limits aren't enforced) |\n",
    "| | Can deploy one model and use everywhere (via API) | Predictions can be slower due to data having to leave device and predictions having to come back (network latency) |\n",
    "| | Links into existing cloud ecosystem | Data has to leave device (this may cause privacy concerns) |\n",
    "\n",
    "There are more details to these but I've left resources in the [extra-curriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#extra-curriculum) to learn more. \n",
    "\n",
    "Let's give an example.\n",
    "\n",
    "If we're deploying FoodVision Mini as an app, we want it to perform well and fast.\n",
    "\n",
    "So which model would we prefer? \n",
    "\n",
    "1. A model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.\n",
    "2. A model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per per prediction (bigger, better model but takes longer to compute).\n",
    "\n",
    "I've made these numbers up but they showcase a potential difference between on-device and on the cloud.\n",
    "\n",
    "Option 1 could potentially be a smaller less performant model that runs fast because its able to fit on a mobile device.\n",
    "\n",
    "Option 2 could potentially a larger more performant model that requires more compute and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual prediction might be fast, the network time and data transfer has to factored in).\n",
    "\n",
    "For FoodVision Mini, we'd likely prefer option 1, because the small hit in performance is far outweighed by the faster inference speed.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-on-device-vs-cloud.png\" width=900 alt=\"tesla computer vision system on device vs on the cloud\"/>\n",
    "\n",
    "*In the case of a Tesla car's computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better that's on the cloud? In this case, you'd much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldn't be worth it (or potentially even impossible with poor signal areas).*\n",
    "\n",
    "> **Note:** For a full example of seeing what it's like to deploy a PyTorch model to an edge device, see the [PyTorch tutorial on achieving real-time inference (30fps+)](https://pytorch.org/tutorials/intermediate/realtime_rpi.html) with a computer vision model on a Raspberry Pi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cd51a",
   "metadata": {},
   "source": [
    "### How's it going to function?\n",
    "\n",
    "Back to the ideal use case, when you deploy your machine learning model, how should it work?\n",
    "\n",
    "As in, would you like predictions returned immediately?\n",
    "\n",
    "Or is it okay for them to happen later?\n",
    "\n",
    "These two scenarios are generally referred to as:\n",
    "\n",
    "* **Online (real-time)** - Predictions/inference happen **immediately**. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fraudulent by a model so the purchase can go through.\n",
    "* **Offline (batch)** - Predictions/inference happen **periodically**. For example, a photos application sorts your images into different categories (such as beach, mealtime, family, friends) whilst your mobile device is plugged into charge.\n",
    "\n",
    "> **Note:** \"Batch\" refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (multiple images being predicted/trained on at once).  \n",
    "\n",
    "The main difference between each being: predictions being made immediately or periodically.\n",
    "\n",
    "Periodically can have a varying timescale too, from every few seconds to every few hours or days.\n",
    "\n",
    "And you can mix and match the two.\n",
    "\n",
    "In the case of FoodVision Mini, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).\n",
    "\n",
    "But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've been doing throughout the previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b238d2c",
   "metadata": {},
   "source": [
    "### Ways to deploy a machine learning model\n",
    "\n",
    "We've discussed a couple of options for deploying machine learning models (on-device and cloud).\n",
    "\n",
    "And each of these will have their specific requirements:\n",
    "\n",
    "| **Tool/resource** | **Deployment type** | \n",
    "| ----- | ----- |\n",
    "| [Google's ML Kit](https://developers.google.com/ml-kit) | On-device (Android and iOS) | \n",
    "| [Apple's Core ML](https://developer.apple.com/documentation/coreml) and [`coremltools` Python package](https://coremltools.readme.io/docs) | On-device (all Apple devices) | \n",
    "| [Amazon Web Service's (AWS) Sagemaker](https://aws.amazon.com/sagemaker/) | Cloud | \n",
    "| [Google Cloud's Vertex AI](https://cloud.google.com/vertex-ai) | Cloud |\n",
    "| [Microsoft's Azure Machine Learning](https://azure.microsoft.com/en-au/services/machine-learning/) | Cloud |\n",
    "| [Hugging Face Spaces](https://huggingface.co/spaces) | Cloud |\n",
    "| API with [FastAPI](https://fastapi.tiangolo.com) | Cloud/self-hosted server |\n",
    "| API with [TorchServe](https://pytorch.org/serve/) | Cloud/self-hosted server | \n",
    "| [ONNX (Open Neural Network Exchange)](https://onnx.ai/index.html) | Many/general |\n",
    "| Many more... ||\n",
    "\n",
    "> **Note:** An [application programming interface (API)](https://en.wikipedia.org/wiki/API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.\n",
    "\n",
    "Which option you choose will be highly dependent on what you're building/who you're working with.\n",
    "\n",
    "But with so many options, it can be very intimidating.\n",
    "\n",
    "So best to start small and keep it simple.\n",
    "\n",
    "And one of the best ways to do so is by turning your machine learning model into a demo app with [Gradio](https://gradio.app) and then deploying it on Hugging Face Spaces.\n",
    "\n",
    "We'll be doing just that with FoodVision Mini later on.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-tools-and-places-to-deploy-ml-models.png\" alt=\"tools and places to deploy machine learning models\" width=900/>\n",
    "\n",
    "*A handful of places and tools to host and deploy machine learning models. There are plenty I've missed so if you'd like to add more, please leave a [discussion on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd6c59",
   "metadata": {},
   "source": [
    "## What we're going to cover \n",
    "\n",
    "Enough talking about deploying a machine learning model.\n",
    "\n",
    "Let's become machine learning engineers and actually deploy one.\n",
    "\n",
    "Our goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics:\n",
    "1. **Performance:** 95%+ accuracy.\n",
    "2. **Speed:** real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s).\n",
    "\n",
    "We'll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.\n",
    "\n",
    "Then we'll deploy the one which performs closest to our goal metrics.\n",
    "\n",
    "Finally, we'll finish with a (BIG) surprise bonus.\n",
    "\n",
    "| **Topic** | **Contents** | \n",
    "| ----- | ----- | \n",
    "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. | \n",
    "| **1. Get data** | Let's download the [`pizza_steak_sushi_20_percent.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) dataset so we can train our previously best performing models on the same dataset. |\n",
    "| **2. FoodVision Mini model deployment experiment outline** | Even on the third milestone project, we're still going to be running multiple experiments to see which model (EffNetB2 or ViT) achieves closest to our goal metrics. |\n",
    "| **3. Creating an EffNetB2 feature extractor** | An EfficientNetB2 feature extractor performed the best on our pizza, steak, sushi dataset in [07. PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/), let's recreate it as a candidate for deployment. |\n",
    "| **4. Creating a ViT feature extractor** | A ViT feature extractor has been the best performing model yet on our pizza, steak, sushi dataset in [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/), let's recreate it as a candidate for deployment alongside EffNetB2. |\n",
    "| **5. Making predictions with our trained models and timing them** | We've built two of the best performing models yet, let's make predictions with them and track their results. |\n",
    "| **6. Comparing model results, prediction times and size** | Let's compare our models to see which performs best with our goals. | \n",
    "| **7. Bringing FoodVision Mini to life by creating a Gradio demo** | One of our models performs better than the other (in terms of our goals), so let's turn it into a working app demo! |\n",
    "| **8. Turning our FoodVision Mini Gradio demo into a deployable app** | Our Gradio app demo works locally, let's prepare it for deployment! |\n",
    "| **9. Deploying our Gradio demo to HuggingFace Spaces** | Let's take FoodVision Mini to the web and make it pubically accessible for all! |\n",
    "| **10. Creating a BIG surprise** | We've built FoodVision Mini, time to step things up a notch. |\n",
    "| **11. Deploying our BIG surprise** | Deploying one app was fun, how about we make it two? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05d636",
   "metadata": {},
   "source": [
    "## 0. Getting setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111c6024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA devices: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from going_modular import GPU_check, data_setup, engine, helper_functions\n",
    "\n",
    "device = GPU_check.GPU_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0077204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.7.1+cu126', '0.22.1+cu126')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca98d16",
   "metadata": {},
   "source": [
    "## 1. Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a66657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data\\pizza_steak_sushi_20_percent directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/pizza_steak_sushi_20_percent')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download pizza, steak, sushi images from GitHub\n",
    "data_20_percent_path = helper_functions.download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                            destination=\"pizza_steak_sushi_20_percent\")\n",
    "\n",
    "data_20_percent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b40f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi_20_percent/train'),\n",
       " WindowsPath('data/pizza_steak_sushi_20_percent/test'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = data_20_percent_path / \"train\"\n",
    "TEST_DIR = data_20_percent_path / \"test\"\n",
    "\n",
    "TRAIN_DIR, TEST_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4cbba",
   "metadata": {},
   "source": [
    "## 2. FoodVision Mini model deployment experiment outline\n",
    "\n",
    "The ideal deployed model FoodVision Mini performs well and fast. \n",
    "\n",
    "We'd like our model to perform as close to real-time as possible.\n",
    "\n",
    "Real-time in this case being ~30FPS (frames per second) because that's [about how fast the human eye can see](https://www.healthline.com/health/human-eye-fps) (there is debate on this but let's just use ~30FPS as our benchmark).\n",
    "\n",
    "And for classifying three different classes (pizza, steak and sushi), we'd like a model that performs at 95%+ accuracy.\n",
    "\n",
    "Of course, higher accuracy would be nice but this might sacrifice speed.\n",
    "\n",
    "So our goals are:\n",
    "\n",
    "1. **Performance** - A model that performs at 95%+ accuracy.\n",
    "2. **Speed** - A model that can classify an image at ~30FPS (0.03 seconds inference time per image, also known as latency).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployments-speed-vs-inference.png\" alt=\"foodvision mini goals in terms of performance and inference time.\" width=750/>\n",
    "\n",
    "*FoodVision Mini deployment goals. We'd like a fast predicting well-performing model (because a slow app is boring).*\n",
    "\n",
    "We'll put an emphasis on speed, meaning, we'd prefer a model performing at 90%+ accuracy at ~30FPS than a model performing 95%+ accuracy at 10FPS.\n",
    "\n",
    "To try and achieve these results, let's bring in our best performing models from the previous sections: \n",
    "\n",
    "1. **EffNetB2 feature extractor** (EffNetB2 for short) - originally created in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models) using [`torchvision.models.efficientnet_b2()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2) with adjusted `classifier` layers.\n",
    "2. **ViT-B/16 feature extractor** (ViT for short) - originally created in [08. PyTorch Paper Replicating section 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset) using [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#vit-b-16) with adjusted `head` layers.\n",
    "    * **Note** ViT-B/16 stands for \"Vision Transformer Base, patch size 16\".\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-two-experiments.png\" alt=\"modelling experiments for foodvision mini deployments, one effnetb2 feature extractor model and a vision transformer feature extractor model\" width=750 />\n",
    "\n",
    "> **Note:** A \"feature extractor model\" often starts with a model that has been pretrained on a dataset similar to your own problem. The pretrained model's base layers are often left frozen (the pretrained patterns/weights stay the same) whilst some of the top (or classifier/classification head) layers get customized to your own problem by training on your own data. We covered the concept of a feature extractor model in [06. PyTorch Transfer Learning section 3.4](https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54cf53",
   "metadata": {},
   "source": [
    "## 3. Creating an EffNetB2 feature extractor\n",
    "\n",
    "We first created an EffNetB2 feature extractor model in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models).\n",
    "\n",
    "And by the end of that section we saw it performed very well.\n",
    "\n",
    "So let's now recreate it here so we can compare its results to a ViT feature extractor trained on the same data.\n",
    "\n",
    "To do so we can:\n",
    "1. Setup the pretrained weights as [`weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights), where \"`DEFAULT`\" means \"best currently available\" (or could use `weights=\"DEFAULT\"`). \n",
    "2. Get the pretrained model image transforms from the weights with the `transforms()` method (we need these so we can convert our images into the same format as the pretrained EffNetB2 was trained on).\n",
    "3. Create a pretrained model instance by passing the weights to an instance of [`torchvision.models.efficientnet_b2`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2).\n",
    "4. Freeze the base layers in the model.\n",
    "5. Update the classifier head to suit our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233b2892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup pretrained EffNetB2 weights\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "# 2. Get EffNetB2 transforms\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "# 3. Setup pretrained model\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=\"DEFAULT\") # could also use weights=\"DEFAULT\"\n",
    "\n",
    "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b302ec4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.3, inplace=True)\n",
       "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad90d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=1408, out_features=3, bias=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64bd9a",
   "metadata": {},
   "source": [
    "### 3.1 Creating a function to make an EffNetB2 feature extractor\n",
    "Looks like our EffNetB2 feature extractor is ready to go, however, since there's quite a few steps involved here, how about we turn the code above into a function we can re-use later?\n",
    "\n",
    "We'll call it `create_effnetb2_model()` and it'll take a customizable number of classes and a random seed parameter for reproducibility.\n",
    "\n",
    "Ideally, it will return an EffNetB2 feature extractor along with its associated transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes: int=3,\n",
    "                        seed: int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # 4. Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # 5. Change classifier head with random seed for reproducibility\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes, bias=True)\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8b0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2_model()\n",
    "\n",
    "# effnetb2, effnetb2_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1ffb6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 3]               --                   Partial\n",
       "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
       "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
       "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
       "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
       "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
       "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
       "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
       "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
       "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
       "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
       "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
       "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
       "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
       "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
       "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
       "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
       "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
       "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
       "├─Sequential (classifier)                                    [1, 1408]            [1, 3]               --                   True\n",
       "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
       "│    └─Linear (1)                                            [1, 1408]            [1, 3]               4,227                True\n",
       "============================================================================================================================================\n",
       "Total params: 7,705,221\n",
       "Trainable params: 4,227\n",
       "Non-trainable params: 7,700,994\n",
       "Total mult-adds (M): 657.64\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 156.80\n",
       "Params size (MB): 30.82\n",
       "Estimated Total Size (MB): 188.22\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print EffNetB2 model summary (uncomment for full output) \n",
    "summary(effnetb2, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24a27f",
   "metadata": {},
   "source": [
    "### 3.2 Creating DataLoaders for EffNetB2 \n",
    "\n",
    "Our EffNetB2 feature extractor is ready, time to create some `DataLoader`s.\n",
    "\n",
    "We can do this by using the [`data_setup.create_dataloaders()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) function we created in [05. PyTorch Going Modular section 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n",
    "\n",
    "We'll use a `batch_size` of 32 and transform our images using the `effnetb2_transforms` so they're in the same format that our `effnetb2` model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3a558ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x20aa381c0a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x20aa381f670>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from going_modular import data_setup\n",
    "\n",
    "train_loader, test_loader, class_names = data_setup.create_dataloader(train_dir=TRAIN_DIR,\n",
    "                                                                    test_dir=TEST_DIR,\n",
    "                                                                    transform= effnetb2_transforms,\n",
    "                                                                    batch_size=32,)\n",
    "\n",
    "train_loader, test_loader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18767ab3",
   "metadata": {},
   "source": [
    "### 3.3 Training EffNetB2 feature extractor\n",
    "\n",
    "Model ready, `DataLoader`s ready, let's train!\n",
    "\n",
    "Just like in [07. PyTorch Experiment Tracking section 7.6](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code), ten epochs should be enough to get good results.\n",
    "\n",
    "We can do so by creating an optimizer (we'll use [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) with a learning rate of `1e-3`), a loss function (we'll use [`torch.nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for multi-class classification) and then passing these as well as our `DataLoader`s to the [`engine.train()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py) function we created in [05. PyTorch Going Modular section 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf95dc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d960115e794a7f8a809234c7cb25eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.96962 | Train accuracy: 53.33%\u001b[0m\n",
      "\u001b[92mTest loss: 0.72201 | Test accuracy: 94.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.70825 | Train accuracy: 86.22%\u001b[0m\n",
      "\u001b[92mTest loss: 0.56852 | Test accuracy: 94.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.55826 | Train accuracy: 86.67%\u001b[0m\n",
      "\u001b[92mTest loss: 0.48725 | Test accuracy: 92.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.50473 | Train accuracy: 90.67%\u001b[0m\n",
      "\u001b[92mTest loss: 0.41583 | Test accuracy: 94.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.43350 | Train accuracy: 87.78%\u001b[0m\n",
      "\u001b[92mTest loss: 0.37076 | Test accuracy: 96.00%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.43328 | Train accuracy: 91.33%\u001b[0m\n",
      "\u001b[92mTest loss: 0.34257 | Test accuracy: 93.33%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.35526 | Train accuracy: 90.89%\u001b[0m\n",
      "\u001b[92mTest loss: 0.32481 | Test accuracy: 95.33%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.34413 | Train accuracy: 89.33%\u001b[0m\n",
      "\u001b[92mTest loss: 0.33753 | Test accuracy: 92.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.40144 | Train accuracy: 90.67%\u001b[0m\n",
      "\u001b[92mTest loss: 0.28614 | Test accuracy: 96.00%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.31204 | Train accuracy: 91.33%\u001b[0m\n",
      "\u001b[92mTest loss: 0.26890 | Test accuracy: 94.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                            lr = 1e-3)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "effnetb2_results = engine.train(model= effnetb2,\n",
    "                                train_dataloader= train_loader,\n",
    "                                test_dataloader= test_loader,\n",
    "                                optimizer= optimizer,\n",
    "                                loss_fn= loss_fn,\n",
    "                                epochs = 10,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db43f0",
   "metadata": {},
   "source": [
    "### 3.4 Inspecting EffNetB2 loss curves \n",
    "\n",
    "Nice!\n",
    "\n",
    "As we saw in 07. PyTorch Experiment Tracking, the EffNetB2 feature extractor model works quite well on our data.\n",
    "\n",
    "Let's turn its results into loss curves to inspect them further.\n",
    "\n",
    "> **Note:** Loss curves are one of the best ways to visualize how your model's performing. For more on loss curves, check out [04. PyTorch Custom Datasets section 8: What should an ideal loss curve look like?](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9f1856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from going_modular.helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnetb2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de33e1",
   "metadata": {},
   "source": [
    "### 3.5 Saving EffNetB2 feature extractor\n",
    "\n",
    "Now we've got a well-performing trained model, let's save it to file so we can import and use it later.\n",
    "\n",
    "To save our model we can use the [`utils.save_model()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/utils.py) function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy).\n",
    "\n",
    "We'll set the `target_dir` to `\"models\"` and the `model_name` to `\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"` (a little comprehensive but at least we know what's going on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239f2d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ✅ Model saved to: models\\09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
     ]
    }
   ],
   "source": [
    "from going_modular.utils import save_model\n",
    "\n",
    "save_model(model= effnetb2, target_dir= \"models\", model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ae641",
   "metadata": {},
   "source": [
    "### 3.6 Checking the size of EffNetB2 feature extractor\n",
    "\n",
    "Since one of our criteria for deploying a model to power FoodVision Mini is **speed** (~30FPS or better), let's check the size of our model.\n",
    "\n",
    "Why check the size?\n",
    "\n",
    "Well, while not always the case, the size of a model can influence its inference speed.\n",
    "\n",
    "As in, if a model has more parameters, it generally performs more operations and each one of these operations requires some computing power.\n",
    "\n",
    "And because we'd like our model to work on devices with limited computing power (e.g. on a mobile device or in a web browser), generally, the smaller the size the better (as long as it still performs well in terms of accuracy).\n",
    "\n",
    "To check our model's size in bytes, we can use Python's [`pathlib.Path.stat(\"path_to_model\").st_size`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat) and then we can convert it (roughly) to megabytes by dividing it by `(1024*1024)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19119d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained EffNetB2 feature extractor model size: 29 MB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6ed6d",
   "metadata": {},
   "source": [
    "### 3.7 Collecting EffNetB2 feature extractor stats\n",
    "\n",
    "We've got a few statistics about our EffNetB2 feature extractor model such as test loss, test accuracy and model size, how about we collect them all in a dictionary so we can compare them to the upcoming ViT feature extractor.\n",
    "\n",
    "And we'll calculate an extra one for fun, total number of parameters.\n",
    "\n",
    "We can do so by counting the number of elements (or patterns/weights) in `effnetb2.parameters()`. We'll access the number of elements in each parameter using the [`torch.numel()`](https://pytorch.org/docs/stable/generated/torch.numel.html) (short for \"number of elements\") method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ab1522b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7705221"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
    "effnetb2_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0362ee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.27,\n",
       " 'test_acc': 94.67,\n",
       " 'number_of_parameters': 7705221,\n",
       " 'model_size (MB)': 29}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary with EffNetB2 statistics\n",
    "effnetb2_stats = {\"test_loss\": round(effnetb2_results[\"test_loss\"][-1], 2),\n",
    "                    \"test_acc\": round(effnetb2_results[\"test_acc\"][-1], 2),\n",
    "                    \"number_of_parameters\": effnetb2_total_params,\n",
    "                    \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e0bec",
   "metadata": {},
   "source": [
    "## 4. Creating a ViT feature extractor\n",
    "\n",
    "Time to continue with our FoodVision Mini modelling experiments.\n",
    "\n",
    "This time we're going to create a ViT feature extractor.\n",
    "\n",
    "And we'll do it in much the same way as the EffNetB2 feature extractor except this time with [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16) instead of `torchvision.models.efficientnet_b2()`.\n",
    "\n",
    "We'll start by creating a function called `create_vit_model()` which will be very similar to `create_effnetb2_model()` except of course returning a ViT feature extractor model and transforms rather than EffNetB2.\n",
    "\n",
    "Another slight difference is that `torchvision.models.vit_b_16()`'s output layer is called `heads` rather than `classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84cc0948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = torchvision.models.vit_b_16()\n",
    "vit.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afdd56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "def create_vit_model(num_classes: int=3,\n",
    "                    seed: int = 42):\n",
    "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
    "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): ViT-B/16 feature extractor model. \n",
    "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
    "    \"\"\"\n",
    "    # Create ViT_B_16 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights = \"DEFAULT\")\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    model.heads = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81081ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit, vit_transforms = create_vit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab069c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 3]               768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "├─Linear (heads)                                             [1, 768]             [1, 3]               2,307                True\n",
       "============================================================================================================================================\n",
       "Total params: 85,800,963\n",
       "Trainable params: 2,307\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (M): 172.47\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 229.20\n",
       "Estimated Total Size (MB): 333.89\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print ViT feature extractor model summary (uncomment for full output)\n",
    "summary(vit, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4ae7f",
   "metadata": {},
   "source": [
    "### 4.1 Create DataLoaders for ViT\n",
    "\n",
    "We've got our ViT model ready, now let's create some `DataLoader`s for it.\n",
    "\n",
    "We'll do this in the same way we did for EffNetB2 except we'll use `vit_transforms` to transform our images into the same format the ViT model was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4983cd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x25f24216d70>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x25f2467c8b0>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from going_modular.data_setup import create_dataloader\n",
    "\n",
    "train_loader_vit, test_loader_vit, class_names = create_dataloader(train_dir=TRAIN_DIR,\n",
    "                                                            test_dir= TEST_DIR,\n",
    "                                                            transform= vit_transforms,\n",
    "                                                            batch_size=32)\n",
    "\n",
    "train_loader_vit, test_loader_vit, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87814a",
   "metadata": {},
   "source": [
    "### 4.2 Training ViT feature extractor\n",
    "\n",
    "You know what time it is...\n",
    "\n",
    "...it's traininggggggg time (sung in the same tune as the song [Closing Time](https://youtu.be/xGytDsqkQY8)). \n",
    "\n",
    "Let's train our ViT feature extractor model for 10 epochs using our `engine.train()` function with `torch.optim.Adam()` and a learning rate of `1e-3` as our optimizer and `torch.nn.CrossEntropyLoss()` as our loss function.\n",
    "\n",
    "We'll use our `set_seeds()` function before training to try and make our results as reproducible as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25514d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7172ecc2533b49e18acd9dbadc45f844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.54572 | Train accuracy: 85.11%\u001b[0m\n",
      "\u001b[92mTest loss: 0.21021 | Test accuracy: 98.00%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.19969 | Train accuracy: 94.22%\u001b[0m\n",
      "\u001b[92mTest loss: 0.11549 | Test accuracy: 98.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.14244 | Train accuracy: 96.00%\u001b[0m\n",
      "\u001b[92mTest loss: 0.09314 | Test accuracy: 98.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.12703 | Train accuracy: 96.89%\u001b[0m\n",
      "\u001b[92mTest loss: 0.08019 | Test accuracy: 98.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.10276 | Train accuracy: 97.11%\u001b[0m\n",
      "\u001b[92mTest loss: 0.07598 | Test accuracy: 98.00%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.08321 | Train accuracy: 97.56%\u001b[0m\n",
      "\u001b[92mTest loss: 0.06708 | Test accuracy: 99.33%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.07930 | Train accuracy: 98.22%\u001b[0m\n",
      "\u001b[92mTest loss: 0.06138 | Test accuracy: 100.00%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.06276 | Train accuracy: 98.67%\u001b[0m\n",
      "\u001b[92mTest loss: 0.05788 | Test accuracy: 99.33%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.05398 | Train accuracy: 99.11%\u001b[0m\n",
      "\u001b[92mTest loss: 0.05684 | Test accuracy: 98.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n",
      "Looked at 0/450 samples\n",
      "Looked at 64/450 samples\n",
      "Looked at 128/450 samples\n",
      "Looked at 192/450 samples\n",
      "Looked at 256/450 samples\n",
      "Looked at 320/450 samples\n",
      "Looked at 384/450 samples\n",
      "Looked at 28/450 samples\n",
      "\n",
      "\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\u001b[94mTrain loss: 0.04903 | Train accuracy: 99.33%\u001b[0m\n",
      "\u001b[92mTest loss: 0.05584 | Test accuracy: 98.67%\u001b[0m\n",
      "\u001b[91m======================================================\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from going_modular.engine import train\n",
    "\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                                lr = 1e-3)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "vit_results = train(model=vit,\n",
    "                    train_dataloader=train_loader_vit,\n",
    "                    test_dataloader=test_loader_vit,\n",
    "                    optimizer=optimizer,\n",
    "                    loss_fn=loss_fn,\n",
    "                    device=device,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c28ef",
   "metadata": {},
   "source": [
    "### 4.3 Inspecting ViT loss curves\n",
    "\n",
    "Alright, alright, alright, ViT model trained, let's get visual and see some loss curves.\n",
    "\n",
    "> **Note:** Don't forget you can see what an ideal set of loss curves should look like in [04. PyTorch Custom Datasets section 8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74cfd0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vit_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoing_modular\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_loss_curves\n\u001b[1;32m----> 3\u001b[0m plot_loss_curves(\u001b[43mvit_results\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vit_results' is not defined"
     ]
    }
   ],
   "source": [
    "from going_modular.helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13046a6c",
   "metadata": {},
   "source": [
    "### 4.4 Saving ViT feature extractor\n",
    "\n",
    "Our ViT model is performing outstanding! \n",
    "\n",
    "So let's save it to file so we can import it and use it later if we wish.\n",
    "\n",
    "We can do so using the `utils.save_model()` function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51f5e9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ✅ Model saved to: models\\09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "from going_modular.utils import save_model\n",
    "\n",
    "save_model(model=vit,\n",
    "            target_dir=\"models\",\n",
    "            model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Checking the size of ViT feature extractor\n",
    "\n",
    "And since we want to compare our EffNetB2 model to our ViT model across a number of characteristics, let's find out its size.\n",
    "\n",
    "To check our model's size in bytes, we can use Python's `pathlib.Path.stat(\"path_to_model\").st_size` and then we can convert it (roughly) to megabytes by dividing it by `(1024*1024)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeacdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Collecting ViT feature extractor stats\n",
    "\n",
    "Let's put together all of our ViT feature extractor model statistics.\n",
    "\n",
    "We saw it in the summary output above but we'll calculate its total number of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4e00b",
   "metadata": {},
   "source": [
    "Woah, that looks like a fair bit more than our EffNetB2!\n",
    "\n",
    "> **Note:** A larger number of parameters (or weights/patterns) generally means a model has a higher *capacity* to learn, whether it actually uses this extra capacity is another story. In light of this, our EffNetB2 model has 7,705,221 parameters where as our ViT model has 85,800,963 (11.1x more) so we could assume that our ViT model has more of a capacity to learn, if given more data (more opportunities to learn). However, this larger capacity to learn ofen comes with an  increased model filesize and a longer time to perform inference.\n",
    "\n",
    "Now let's create a dictionary with some important characteristics of our ViT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed7d7b",
   "metadata": {},
   "source": [
    "## 5. Making predictions with our trained models and timing them\n",
    "\n",
    "We've got a couple of trained models, both performing pretty well.\n",
    "\n",
    "Now how about we test them out doing what we'd like them to do?\n",
    "\n",
    "As in, let's see how they go making predictions (performing inference).\n",
    "\n",
    "We know both of our models are performing at over 95% accuracy on the test dataset, but how fast are they?\n",
    "\n",
    "Ideally, if we're deploying our FoodVision Mini model to a mobile device so people can take photos of their food and identify it, we'd like the predictions to happen at real-time (~30 frames per second).\n",
    "\n",
    "That's why our second criteria is: a fast model.\n",
    "\n",
    "To find out how long each of our models take to performance inference, let's create a function called `pred_and_store()` to iterate over each of the test dataset images one by one and perform a prediction. \n",
    "\n",
    "We'll time each of the predictions as well as store the results in a common prediction format: a list of dictionaries (where each element in the list is a single prediction and each sinlge prediction is a dictionary). \n",
    "\n",
    "> **Note:** We time the predictions one by one rather than by batch because when our model is deployed, it will likely only be making a prediction on one image at a time. As in, someone takes a photo and our model predicts on that single image.\n",
    "\n",
    "Since we'd like to make predictions across all the images in the test set, let's first get a list of all of the test image paths so we can iterate over them. \n",
    "\n",
    "To do so, we'll use Python's [`pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))`](https://docs.python.org/3/library/pathlib.html#basic-use) to find all of the filepaths in a target directory with the extension `.jpg` (all of our test images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307a299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
