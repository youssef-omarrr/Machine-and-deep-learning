{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixSi6udtrYZk"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/02-langsmith.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NwdBTO0qCQi"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swum099VqCQj"
      },
      "source": [
        "# LangSmith Starter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hk437ZjqCQj"
      },
      "source": [
        "LangSmith is a built-in observability service and platform that integrates _very easily_ with LangChain. You don't _need_ to use LangSmith for this course, but it can be very helpful in understanding what is happening, _and_ we recommend using it beyond this course for general development with LangChain — with all of that in mind we would recommend spending a little bit of time to get familiar with LangSmith."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k9EfPofqG3g",
        "outputId": "71873d67-b511-40b8-9524-d1b0e0ddd3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.3/333.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-openai==0.3.3 \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UdyCjmqqCQk"
      },
      "source": [
        "## Setting up LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqURpTWbqCQk"
      },
      "source": [
        "LangSmith does require an API key, but it comes with a generous free tier. You can sign up an account and get your API key [here](https://smith.langchain.com).\n",
        "\n",
        "When using LangSmith, we need to setup our environment variables _and_ provide our API key, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezoLeIGrqCQk",
        "outputId": "252aa252-d8c3-4cd5-fb18-acf3cad2dee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# must enter API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-langsmith-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGDntgWqCQk"
      },
      "source": [
        "In most cases, this is all we need to start seeing logs and traces in the [LangSmith UI](https://smith.langchain.com). By default, LangChain will trace LLM calls, chains, etc. We'll take a look at a quick example of this below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFDeEk0tqCQk"
      },
      "source": [
        "## Default Tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3hqGO2qCQk"
      },
      "source": [
        "As mentioned, LangSmith traces a lot of data without us needing to do anything. Let's see how that looks. We'll start by initializing our LLM. Again, this will need an [API key](https://platform.openai.com/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AfToUsEqCQl",
        "outputId": "4426952f-af46-440d-d275-a86610a9c1fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
        "    \"Enter OpenAI API Key: \"\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAVDqCttqCQl"
      },
      "source": [
        "Let's invoke our LLM and then see what happens in the LangSmith UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCGtll-IqCQl",
        "outputId": "0f9ea347-b50f-481c-c237-fd1fd6a09589"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-e73b0928-0ea5-4e88-90f7-28470b499ae2-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYM-pIIaqCQl"
      },
      "source": [
        "After this we should see that a new project (`aurelioai-langchain-course-langsmith-openai`) has been created in the LangSmith UI. Inside that project, we should see the trace from our LLM call:\n",
        "\n",
        "\n",
        "![LangSmith LLM Trace](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-01.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7VktC46qCQl"
      },
      "source": [
        "By default, LangSmith will capture plenty — however, it won't capture functions from outside of LangChain. Let's see how we can trace those."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNQRKbp9qCQl"
      },
      "source": [
        "## Tracing Non-LangChain Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbYn3yY7qCQl"
      },
      "source": [
        "LangSmith can trace functions that are not part of LangChain, we just need to add the `@traceable` decorator. Let's try this for a few simple functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NVo50uq6qCQl"
      },
      "outputs": [],
      "source": [
        "from langsmith import traceable\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "@traceable\n",
        "def generate_random_number():\n",
        "    return random.randint(0, 100)\n",
        "\n",
        "@traceable\n",
        "def generate_string_delay(input_str: str):\n",
        "    number = random.randint(1, 5)\n",
        "    time.sleep(number)\n",
        "    return f\"{input_str} ({number})\"\n",
        "\n",
        "@traceable\n",
        "def random_error():\n",
        "    number = random.randint(0, 1)\n",
        "    if number == 0:\n",
        "        raise ValueError(\"Random error\")\n",
        "    else:\n",
        "        return \"No error\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx4b1yJtqCQl"
      },
      "source": [
        "Let's run these a few times and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2af699bd631846bdb2e064981a48ae67",
            "1507edeb2bcf43238ed2431731b51f24",
            "76c4884af2ed4474a07a6f8964cd7b10",
            "302271efa18349fb8fd20e2fa922f17f",
            "59d44325b22241a0828dd1df4c87eb46",
            "fb6ca9c9ad144bc3947e454cc3f3ab77",
            "1e91ab27a61c4530ac1dd2e05a4455ae",
            "a087fea20eb040469312b40081517d67",
            "ba1df7e62b114c6d9062c65a04bb349d",
            "e7967f66202744d1b41ec3ae02a45617",
            "fe1edcf8e1fa4ae2bdae23d2235395e0"
          ]
        },
        "id": "iTDAR7b4qCQl",
        "outputId": "084ca65c-2497-41cf-e6f6-5b946163b5f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2af699bd631846bdb2e064981a48ae67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for _ in tqdm(range(10)):\n",
        "    generate_random_number()\n",
        "    generate_string_delay(\"Hello\")\n",
        "    try:\n",
        "        random_error()\n",
        "    except ValueError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCoeNfRiqCQl"
      },
      "source": [
        "Those traces should now be visible in the LangSmith UI, again under the same project:\n",
        "\n",
        "![LangSmith Traces](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-02.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuze3LTqCQl"
      },
      "source": [
        "We can various metrics here for each run. First, ofcourse, the run name. We can see any inputs and outputs from each run, we can see if the run raised any errors, it's start time, and latency. Inside the UI we can also filter for specific runs, like so:\n",
        "\n",
        "![LangSmith UI filters](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-03.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7UR0DPwqCQl"
      },
      "source": [
        "There are various other things we can do within the UI, but we'll leave that for you to explore.\n",
        "\n",
        "Finally, we can also modify our traceable names if we'd like to make them more readable inside the UI. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dQLkwo0PqCQl"
      },
      "outputs": [],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "@traceable(name=\"Chitchat Maker\")\n",
        "def error_generation_function(question: str):\n",
        "    delay = random.randint(0, 3)\n",
        "    time.sleep(delay)\n",
        "    number = random.randint(0, 1)\n",
        "    if number == 0:\n",
        "        raise ValueError(\"Random error\")\n",
        "    else:\n",
        "        return \"I'm great how are you?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8NFR6qqCQl"
      },
      "source": [
        "Let's run this a few times and see what we get in LangSmith."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e988534598c54261a671bc9e17fdb81b",
            "4c3b5008fb254259bfded2cffaa43790",
            "5e63efa23bd54da5bc94a6776f11d0e7",
            "15c2401e19a34ce48286460dad8bf4b2",
            "c884ebe0266b47aa9620600ed2169c81",
            "a27c7c05a70041fcb8dfab45692ef7fc",
            "c1bc2c068ee840d38a4b615f108e4e0c",
            "ffce1b8c4b8646c2b00f81058cb6e6c0",
            "786d6b07a8104f5fa499fbf345bc831f",
            "479c419c88a3443b8c39767fb3540c8b",
            "290f779d73ca42c9bd826518d686bbea"
          ]
        },
        "id": "ay8sxvXWqCQm",
        "outputId": "bafa13da-6ffe-48cb-fa5c-4e18e65a6e3b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e988534598c54261a671bc9e17fdb81b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for _ in tqdm(range(10)):\n",
        "    try:\n",
        "        error_generation_function(\"How are you today?\")\n",
        "    except ValueError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsITHuXBqCQm"
      },
      "source": [
        "Let's filter for the `Chitchat Maker` traceable and see our results.\n",
        "\n",
        "![LangSmith Chitchat Maker runs](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-04.jpg?raw=1)\n",
        "\n",
        "We can see our runs and their related metadata! That's it for this intro to LangSmith. As we work through the course we will (optionally) refer to LangSmith for digging into our runs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
