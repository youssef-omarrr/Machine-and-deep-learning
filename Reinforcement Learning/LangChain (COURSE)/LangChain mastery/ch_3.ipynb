{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b07633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must enter API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# below should not be changed\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# you can change this as preferred\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-langsmith-groq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55d399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\",\n",
    "            temperature = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69aa096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c33521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# passing the template to the LangChain model\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad0e5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360f7e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt), # -> prompt and context\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\") # -> user input\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e123ab1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "429c9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\": lambda x: x[\"context\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1efe94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779bc67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Aurelio\\u202fAI is an AI‑focused company that builds tools and services for AI engineers, especially those working with language models. Their core activities include:\\n\\n* **Developing open‑source frameworks** – most notably **Semantic\\u202fRouter** and **Semantic\\u202fChunkers**, which help with routing and chunking of textual data.\\n* **Providing an AI Platform** – a suite of tooling that streamlines the end‑to‑end development, deployment, and management of language‑model‑based applications.\\n* **Offering development services** – helping other organizations design, build, and bring AI‑driven products to market, leveraging their deep expertise in AI agents and information‑retrieval techniques.\\n* **LangChain expertise** – since September\\u202f2024, Aurelio\\u202fAI has been recognized as a LangChain Expert, reflecting a strong track record of delivering solutions built on the LangChain ecosystem.', additional_kwargs={'reasoning_content': 'We need to answer based on context. Summarize: AI company developing tooling for AI engineers, focus on language AI, expertise in AI agents and information retrieval. Open source frameworks: Semantic Router, Semantic Chunkers. AI Platform providing tooling. Development services. Became LangChain Experts in Sep 2024. So answer accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 258, 'prompt_tokens': 245, 'total_tokens': 503, 'completion_time': 0.538224083, 'completion_tokens_details': {'reasoning_tokens': 68}, 'prompt_time': 0.009870938, 'prompt_tokens_details': None, 'queue_time': 0.01752784, 'total_time': 0.548095021}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_87b3c396db', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c82b9-1c98-7873-82fa-26bc3d703ea2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 245, 'output_tokens': 258, 'total_tokens': 503, 'output_token_details': {'reasoning': 68}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.invoke({\"query\": query, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26f9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37ad969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Here is query #1\n",
      "AI: Here is the answer #1\n",
      "Human: Here is query #2\n",
      "AI: Here is the answer #2\n",
      "Human: Here is query #3\n",
      "AI: Here is the answer #3\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "# here is the formatted prompt\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd5df565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Overview  \n",
      "Aurelio AI is an AI‑focused company that builds tools and services for **AI engineers**, especially those working with **language models** and **AI agents**.\n",
      "\n",
      "### What Aurelio AI Offers\n",
      "- **Open‑source frameworks**\n",
      "  - **Semantic Router** – helps route queries and responses based on semantic similarity.  \n",
      "  - **Semantic Chunkers** – splits text into meaningful chunks for retrieval‑augmented generation.\n",
      "- **AI Platform**\n",
      "  - Provides a suite of tooling that streamlines the development, testing, and deployment of language‑model‑centric applications.\n",
      "- **Professional Services**\n",
      "  - Offers development and consulting services to help organizations bring their AI products to market quickly and reliably.\n",
      "- **LangChain Expertise**\n",
      "  - Recognized as LangChain Experts (since September 2024) with a proven track record of delivering solutions built on the LangChain ecosystem.\n",
      "\n",
      "### Core Focus Areas\n",
      "- **Language AI** – building and optimizing large‑language‑model (LLM) applications.  \n",
      "- **AI Agents** – designing autonomous agents that can reason, retrieve information, and act.  \n",
      "- **Information Retrieval** – leveraging semantic search and chunking to improve data access for LLMs.\n",
      "\n",
      "## Conclusion  \n",
      "Aurelio AI equips AI engineers with open‑source tools, a robust platform, and expert services to accelerate the creation of language‑model‑driven applications, while also providing deep expertise in LangChain and information retrieval.\n"
     ]
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template.messages[0].prompt.template = new_system_prompt\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba6a54eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Overview  \n",
       "Aurelio AI is an AI‑focused company that builds tools and services for **AI engineers**, especially those working with **language models** and **AI agents**.\n",
       "\n",
       "### What Aurelio AI Offers\n",
       "- **Open‑source frameworks**\n",
       "  - **Semantic Router** – helps route queries and responses based on semantic similarity.  \n",
       "  - **Semantic Chunkers** – splits text into meaningful chunks for retrieval‑augmented generation.\n",
       "- **AI Platform**\n",
       "  - Provides a suite of tooling that streamlines the development, testing, and deployment of language‑model‑centric applications.\n",
       "- **Professional Services**\n",
       "  - Offers development and consulting services to help organizations bring their AI products to market quickly and reliably.\n",
       "- **LangChain Expertise**\n",
       "  - Recognized as LangChain Experts (since September 2024) with a proven track record of delivering solutions built on the LangChain ecosystem.\n",
       "\n",
       "### Core Focus Areas\n",
       "- **Language AI** – building and optimizing large‑language‑model (LLM) applications.  \n",
       "- **AI Agents** – designing autonomous agents that can reason, retrieve information, and act.  \n",
       "- **Information Retrieval** – leveraging semantic search and chunking to improve data access for LLMs.\n",
       "\n",
       "## Conclusion  \n",
       "Aurelio AI equips AI engineers with open‑source tools, a robust platform, and expert services to accelerate the creation of language‑model‑driven applications, while also providing deep expertise in LangChain and information retrieval."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeef528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without any other\n",
    "text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", no_cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d6d26d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18893\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"How many keystrokes are needed to type the numbers from 1 to 5000?\"\n",
    ")\n",
    "\n",
    "no_cot_pipeline = no_cot_prompt_template | llm\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
    "print(no_cot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b8f0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  subproblems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "cot_pipeline = cot_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac105fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Step‑by‑step breakdown**\n",
       "\n",
       "| Step | What we need to do | Numbers involved | How many numbers | Digits per number | Keystrokes contributed |\n",
       "|------|-------------------|------------------|------------------|-------------------|------------------------|\n",
       "| 1 | Identify the digit‑length groups that appear between 1 and 5000. | – | – | – | The groups are 1‑digit (1‑9), 2‑digit (10‑99), 3‑digit (100‑999), 4‑digit (1000‑5000). |\n",
       "| 2 | Count how many 1‑digit numbers are in the range. | 1 – 9 | \\(9\\) | 1 | \\(9 \\times 1 = 9\\) |\n",
       "| 3 | Count how many 2‑digit numbers are in the range. | 10 – 99 | \\(99-10+1 = 90\\) | 2 | \\(90 \\times 2 = 180\\) |\n",
       "| 4 | Count how many 3‑digit numbers are in the range. | 100 – 999 | \\(999-100+1 = 900\\) | 3 | \\(900 \\times 3 = 2\\,700\\) |\n",
       "| 5 | Count how many 4‑digit numbers are in the range **up to 4999**. | 1000 – 4999 | \\(4999-1000+1 = 4\\,000\\) | 4 | \\(4\\,000 \\times 4 = 16\\,000\\) |\n",
       "| 6 | Add the final number, 5000, which is also 4‑digit. | 5000 | \\(1\\) | 4 | \\(1 \\times 4 = 4\\) |\n",
       "| 7 | Sum all contributions. | – | – | – | \\(9 + 180 + 2\\,700 + 16\\,000 + 4 = 18\\,893\\) |\n",
       "\n",
       "**Final answer**\n",
       "\n",
       "The total number of keystrokes required to type every integer from **1** through **5000** (counting each digit as one keystroke) is  \n",
       "\n",
       "\\[\n",
       "\\boxed{18\\,893\\ \\text{keystrokes}}.\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749e721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
