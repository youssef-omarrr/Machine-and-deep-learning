{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31784dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\",\n",
    "            temperature = 0.2)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0b1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ( \n",
    "                                    SystemMessagePromptTemplate,\n",
    "                                    HumanMessagePromptTemplate,\n",
    "                                    MessagesPlaceholder,\n",
    "                                    ChatPromptTemplate )\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c650f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2aea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "        \n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        # ConfigurableFieldSpec(\n",
    "        #     id=\"k\",\n",
    "        #     annotation=int,\n",
    "        #     name=\"k\",\n",
    "        #     description=\"The number of messages to keep in the history\",\n",
    "        #     default=4,\n",
    "        # )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41b8db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you again, James! How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi, my name is James\". The assistant should respond appropriately, maybe greet again. There\\'s no conflict. Just respond.'}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 141, 'total_tokens': 194, 'completion_time': 0.109363555, 'completion_tokens_details': {'reasoning_tokens': 29}, 'prompt_time': 0.005644656, 'prompt_tokens_details': None, 'queue_time': 0.019078482, 'total_time': 0.115008211}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_96d96a151c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c82c6-34c6-7f82-95be-3874fb9a93dd-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 141, 'output_tokens': 53, 'total_tokens': 194, 'output_token_details': {'reasoning': 29}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eaf0253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={'reasoning_content': 'The user is asking \"What is my name again?\" The assistant should answer \"Your name is James.\" There\\'s no policy violation. Just answer.'}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 172, 'total_tokens': 216, 'completion_time': 0.090729483, 'completion_tokens_details': {'reasoning_tokens': 30}, 'prompt_time': 0.006564051, 'prompt_tokens_details': None, 'queue_time': 0.017671839, 'total_time': 0.097293534}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_87b3c396db', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c82c6-379e-7112-9941-c666ba6784bb-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 172, 'output_tokens': 44, 'total_tokens': 216, 'output_token_details': {'reasoning': 30}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e004464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "        \n",
    "        \n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84b022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatGroq = Field(default_factory=ChatGroq)\n",
    "\n",
    "    def __init__(self, llm: ChatGroq):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        \n",
    "        # construct the summary chat messages\n",
    "        # =====================================\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages.content,\n",
    "                messages=[x.content for x in messages]\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "        \n",
    "        \n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatGroq) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b380222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
