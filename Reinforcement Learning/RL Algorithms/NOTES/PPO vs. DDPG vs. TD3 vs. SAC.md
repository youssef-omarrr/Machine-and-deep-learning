
| Category                       | **PPO**                                                                                                     | **DDPG**                                                                                               | **TD3**                                                 | **SAC**                                                                             |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| **Algorithm Type**             | On-policy Actor-Critic (Policy Gradient)                                                                    | Off-policy Actor-Critic                                                                                | Off-policy Actor-Critic                                 | Off-policy Entropy-regularized Actor-Critic                                         |
| **Action Space**               | Continuous & Discrete                                                                                       | Continuous                                                                                             | Continuous                                              | Continuous (also discrete variant)                                                  |
| **Policy Type**                | Stochastic (gives a **probability distribution over actions**, so the action can vary each time you sample) | Deterministic (Every time you give the same state s to the policy, it produces the **same action** a.) | Deterministic                                           | Stochastic                                                                          |
| **Exploration Method**         | *Sampling* from stochastic policy                                                                           | Add external noise (OU or Gaussian)                                                                    | Gaussian noise + target policy smoothing                | Built-in stochastic policy (entropy) (sample action using reparameterization trick) |
| **Q-Networks**                 | Optional value function baseline, no Q-network                                                              | 1 Critic                                                                                               | 2 Critics (Clipped Double Q)                            | 2 Critics (Clipped Double Q)                                                        |
| **Value Estimation**           | Generalized Advantage Estimation (GAE)                                                                      | Q-learning                                                                                             | Clipped Double Q-learning                               | Clipped Double Q-learning + entropy                                                 |
| **Stability Mechanisms**       | Clipped Objective + Trust Region–like update                                                                | Replay + Target Networks                                                                               | Replay + Target Nets + Double Critics + Delayed Updates | Replay + Double Critics + Entropy + α tuning                                        |
| **Target Networks**            | No                                                                                                          | Yes                                                                                                    | Yes (both actors & critics)                             | Yes (critics only)                                                                  |
| **Objective Function**         | Maximize clipped policy ratio objective                                                                     | Maximize Q(s, μ(s))                                                                                    | Same as DDPG but more stable targets                    | Maximize reward + entropy                                                           |
| **Key Formula**                | $$L=\min(r_t(\theta)\hat{A}_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)$$                  | $$y=r+\gamma Q'(s',\mu'(s'))$$                                                                         | $$y=r+\gamma \min(Q'_1,Q'_2)$$                          | $$y=r+\gamma (\min(Q'_1,Q'_2)-\alpha \log \pi(a'\|s'))$$                            |
| **Temperature Parameter α**    | None                                                                                                        | None                                                                                                   | None                                                    | Learned automatically                                                               |
| **Policy Update Frequency**    | Every few epochs on same batch (multiple passes)                                                            | Every step (same as critic)                                                                            | Less frequent (delayed by k steps)                      | Every step                                                                          |
| **Replay Buffer**              | Not used                                                                                                    | Required                                                                                               | Required                                                | Required                                                                            |
| **On-policy or Off-policy**    | On-policy                                                                                                   | Off-policy                                                                                             | Off-policy                                              | Off-policy                                                                          |
| **Sample Efficiency**          | Low (needs many interactions)                                                                               | High                                                                                                   | High                                                    | Very High                                                                           |
| **Learning Stability**         | High                                                                                                        | Medium (unstable)                                                                                      | High                                                    | Very High                                                                           |
| **Overestimation Handling**    | Not applicable (no Q-learning)                                                                              | None                                                                                                   | Double critic mins                                      | Double critic mins + entropy smoothing                                              |
| **Entropy Maximization**       | Optional (via entropy bonus)                                                                                | No                                                                                                     | No                                                      | Yes (core idea)                                                                     |
| **Action Noise**               | Stochastic policy                                                                                           | OU (Ornstein-Uhlenbeck) Noise / Gaussian                                                               | Gaussian Noise + Smoothing                              | Inherent stochasticity                                                              |
| **Hyperparameter Sensitivity** | Medium                                                                                                      | High                                                                                                   | Medium                                                  | Low (α auto-tuned)                                                                  |
| **Computational Cost**         | Medium                                                                                                      | Low                                                                                                    | Medium                                                  | High                                                                                |
| **Update Style**               | Clipped policy gradient + GAE                                                                               | Deterministic PG                                                                                       | Deterministic PG + TD3 tricks                           | Soft Q-learning + reparameterization                                                |
| **Best For**                   | General RL tasks, widely used in robotics & games                                                           | Simple continuous-control tasks                                                                        | Harder continuous tasks, robotics                       | Most continuous control benchmarks; SOTA                                            |
| **Major Weakness**             | Poor sample efficiency (on-policy)                                                                          | Overestimation + instability                                                                           | Relatively slow updates due to delays                   | Higher compute cost                                                                 |
| **Typical Benchmarks**         | Atari, MuJoCo, robotics                                                                                     | Pendulum, basic robotics                                                                               | MuJoCo, robotics, locomotion                            | MuJoCo, robotics, OpenAI Gym                                                        |
| **Implementation Complexity**  | Medium                                                                                                      | Low–Medium                                                                                             | Medium                                                  | High                                                                                |
| **Reparameterization Trick**   | Commonly used (Gaussian policies)                                                                           | Optional in some versions                                                                              | Optional                                                | Required                                                                            |
| **Environment Compatibility**  | Both                                                                                                        | Continuous only                                                                                        | Continuous only                                         | Continuous + discrete                                                               |
| **Policy Output**              | Mean + Std (Gaussian)                                                                                       | Action value                                                                                           | Action value                                            | Mean + Std (Gaussian)                                                               |
| **Update Target Frequency**    | Not used                                                                                                    | Soft updates each step                                                                                 | Same but with delays                                    | Soft updates                                                                        |
| **Advantage Estimation**       | GAE: yes                                                                                                    | Not used                                                                                               | Not used                                                | Not used                                                                            |
| **Training Stability Ranking** | High                                                                                                        | Lowest                                                                                                 | High                                                    | Highest                                                                             |
| **Sample Efficiency Ranking**  | Lowest                                                                                                      | Medium                                                                                                 | High                                                    | Highest                                                                             |
| **Overall Practical Ranking**  | General-purpose, stable                                                                                     | Outdated                                                                                               | Reliable                                                | State-of-the-art                                                                    |
