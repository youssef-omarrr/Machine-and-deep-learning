{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3c8515",
   "metadata": {},
   "source": [
    "# Version 3: with the help of Claude code to find the state of the art solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edaba0",
   "metadata": {},
   "source": [
    "## 0. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "475680c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from monai.losses import DiceCELoss, FocalLoss, TverskyLoss\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf0c70",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "This project uses a custom `BrainTumorDataset` class, built on `torch.utils.data.Dataset`, to efficiently load and preprocess the brain MRI data.\n",
    "\n",
    "Key features of the data pipeline:\n",
    "\n",
    "- **File Handling:** It automatically discovers image-mask pairs from separate directories, ensuring that only images with corresponding masks are used for training.\n",
    "- **Preprocessing:**\n",
    "    - Images are loaded and converted to a standard format (RGB or grayscale).\n",
    "    - Both images and masks are resized to a uniform dimension (`512x512`) to be compatible with the model input. It correctly uses `INTER_NEAREST` interpolation for masks to preserve sharp boundaries.\n",
    "    - Masks are binarized (0 for background, 1 for tumor) to create a clear ground truth for segmentation.\n",
    "- **Augmentation:** The dataset is designed to integrate seamlessly with data augmentation libraries like `albumentations`. It handles the conversion of grayscale images to a 3-channel format required by many standard augmentation transforms.\n",
    "- **Tensor Conversion:** Final images and masks are converted into PyTorch tensors, with masks specifically cast to `long` type, making them ready for use with common loss functions like `CrossEntropyLoss` or `DiceLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e73242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for brain tumor segmentation.\n",
    "    Loads images and masks, applies transformations, and prepares them for the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, transform=None, image_size=(512, 512)):\n",
    "        # 1. Initialize paths and parameters\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # 2. Get all image files from the directory\n",
    "        self.image_files = sorted( list(self.images_dir.glob(\"*.png\")) )\n",
    "        \n",
    "        # 3. Filter files to ensure each image has a corresponding mask\n",
    "        self.valid_files = []\n",
    "        for img_path in self.image_files:\n",
    "            mask_path = self.masks_dir / img_path.name\n",
    "            if mask_path.exists():\n",
    "                self.valid_files.append(img_path)\n",
    "        \n",
    "        print(f\"Found {len(self.valid_files)} valid image-mask pairs\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the total number of valid samples\n",
    "        return len(self.valid_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # --- 1. Load files ---\n",
    "        img_path = self.valid_files[idx]\n",
    "        mask_path = self.masks_dir / img_path.name\n",
    "        \n",
    "        # --- 2. Read image and mask ---\n",
    "        # Read image and convert from BGR (OpenCV default) to RGB\n",
    "        image = cv2.imread(str(img_path))\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not load image: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Read mask as a single-channel grayscale image\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Could not load mask: {mask_path}\")\n",
    "        \n",
    "        # --- 3. Preprocessing ---\n",
    "        # Resize image and mask to a consistent size\n",
    "        if image.shape[:2] != self.image_size:\n",
    "            # Use linear interpolation for the image\n",
    "            image = cv2.resize(image, self.image_size, interpolation=cv2.INTER_LINEAR)\n",
    "            # Use nearest-neighbor interpolation for the mask to preserve binary values\n",
    "            mask = cv2.resize(mask, self.image_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Convert to grayscale if the image is 3-channel but has identical channels\n",
    "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "            # Check if it's actually grayscale (all channels equal)\n",
    "            if np.allclose(image[:,:,0], image[:,:,1]) and np.allclose(image[:,:,1], image[:,:,2]):\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Binarize the mask: pixels > 127 become 1 (tumor), others 0 (background)\n",
    "        mask = (mask > 127).astype(np.uint8)\n",
    "        \n",
    "        # --- 4. Apply augmentations ---\n",
    "        if self.transform:\n",
    "            # Convert grayscale to 3-channel for compatibility with some albumentations transforms\n",
    "            if len(image.shape) == 2:\n",
    "                image = np.stack([image, image, image], axis=-1)\n",
    "            \n",
    "            # Apply the provided transformation pipeline\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # --- 5. Final conversion for PyTorch ---\n",
    "        # Convert mask to a LongTensor for the loss function (e.g., CrossEntropyLoss)\n",
    "        mask = mask.long()\n",
    "        \n",
    "        # Return a dictionary containing the processed data\n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'filename': img_path.name\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e04491",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation Strategy\n",
    "To build a robust segmentation model, a comprehensive data augmentation strategy is employed using the `albumentations` library. Separate pipelines are defined for the training and validation datasets to ensure proper model training and unbiased evaluation.\n",
    "\n",
    "**Training Augmentations (`get_train_transforms`)**\n",
    "\n",
    "The training pipeline applies a wide variety of augmentations to teach the model to be invariant to changes in position, orientation, and imaging conditions. This helps prevent overfitting and improves generalization to unseen data. Key transformations include:\n",
    "\n",
    "- **Geometric Transforms:** Random rotations, flips, transpositions, and a combined shift-scale-rotate transform to handle variations in patient positioning.\n",
    "- **Elastic Deformations:** ElasticTransform and GridDistortion are used to simulate the non-rigid nature of biological tissue, a critical augmentation for medical imaging.\n",
    "- **Intensity and Noise:** Adjustments to brightness, contrast, and gamma, along with the addition of Gaussian noise and blur, mimic differences between MRI scanners and acquisition protocols.\n",
    "- **Contrast Enhancement:** CLAHE (Contrast Limited Adaptive Histogram Equalization) is applied to enhance local contrast, making subtle tumor boundaries more apparent.\n",
    "- **Final Preprocessing:** Images are normalized using standard ImageNet statistics and converted to PyTorch tensors.\n",
    "\n",
    "\n",
    "**Validation Preprocessing (`get_val_transforms`)**\n",
    "\n",
    "The validation pipeline is minimal and deterministic. It contains **no random augmentations**. This is crucial for obtaining a stable and reliable measure of the model's performance. The only steps are:\n",
    "\n",
    "1. **Normalization**: To ensure the validation data is in the same value range as the training data.\n",
    "1. **Tensor Conversion**: To prepare the data for model input.\n",
    "\n",
    "This dual-pipeline approach ensures the model learns from a diverse set of examples while being evaluated on a consistent, unaltered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0849a183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Compose([\n",
       "   RandomRotate90(p=0.5),\n",
       "   HorizontalFlip(p=0.5),\n",
       "   VerticalFlip(p=0.5),\n",
       "   Transpose(p=0.5),\n",
       "   ShiftScaleRotate(p=0.5, shift_limit_x=(-0.0625, 0.0625), shift_limit_y=(-0.0625, 0.0625), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-15.0, 15.0), interpolation=1, border_mode=0, fill=0.0, fill_mask=0.0, rotate_method='largest_box', mask_interpolation=0),\n",
       "   ElasticTransform(p=0.3, alpha=1.0, approximate=False, border_mode=0, fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, noise_distribution='gaussian', same_dxdy=False, sigma=50.0),\n",
       "   GridDistortion(p=0.3, border_mode=0, distort_limit=(-0.3, 0.3), fill=0.0, fill_mask=0.0, interpolation=1, keypoint_remapping_method='mask', mask_interpolation=0, normalized=True, num_steps=5),\n",
       "   RandomBrightnessContrast(p=0.5, brightness_by_max=True, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), ensure_safe_range=False),\n",
       "   RandomGamma(p=0.3, gamma_limit=(80.0, 120.0)),\n",
       "   GaussNoise(p=0.3, mean_range=(0.0, 0.0), noise_scale_factor=1.0, per_channel=True, std_range=(0.2, 0.44)),\n",
       "   GaussianBlur(p=0.2, blur_limit=(0, 3), sigma_limit=(0.5, 3.0)),\n",
       "   CLAHE(p=0.3, clip_limit=(1.0, 2.0), tile_grid_size=(8, 8)),\n",
       "   Normalize(p=1.0, max_pixel_value=255.0, mean=(0.485, 0.456, 0.406), normalization='standard', std=(0.229, 0.224, 0.225)),\n",
       "   ToTensorV2(p=1.0, transpose_mask=False),\n",
       " ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True),\n",
       " Compose([\n",
       "   Normalize(p=1.0, max_pixel_value=255.0, mean=(0.485, 0.456, 0.406), normalization='standard', std=(0.229, 0.224, 0.225)),\n",
       "   ToTensorV2(p=1.0, transpose_mask=False),\n",
       " ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_train_transforms():\n",
    "    \"\"\"\n",
    "    Defines the augmentation pipeline for the training dataset.\n",
    "    Uses a rich set of transforms to improve model robustness and generalization.\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # --- 1. Geometric Transformations ---\n",
    "        # These alter the spatial orientation of the image.\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Transpose(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.0625,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode=cv2.BORDER_CONSTANT, # Fill new pixels with a constant value\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "            p=0.5\n",
    "        ),\n",
    "\n",
    "        # --- 2. Non-rigid (Elastic) Transformations ---\n",
    "        # These simulate tissue deformation, which is common in medical scans.\n",
    "        A.ElasticTransform(\n",
    "            alpha=1,\n",
    "            sigma=50,\n",
    "            alpha_affine=50,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "            p=0.3\n",
    "        ),\n",
    "        A.GridDistortion(\n",
    "            num_steps=5,\n",
    "            distort_limit=0.3,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "            p=0.3\n",
    "        ),\n",
    "\n",
    "        # --- 3. Intensity and Color Transformations ---\n",
    "        # These alter pixel values to simulate different lighting/scanner conditions.\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2,\n",
    "            contrast_limit=0.2,\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "\n",
    "        # --- 4. Noise and Blurring ---\n",
    "        # Simulates sensor noise and minor focus issues.\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "        A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "\n",
    "        # --- 5. Advanced Contrast Enhancement ---\n",
    "        # CLAHE is highly effective for enhancing local contrast in medical images.\n",
    "        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),\n",
    "\n",
    "        # --- 6. Final Preprocessing Steps ---\n",
    "        # Normalize the image using ImageNet stats (a common starting point).\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        # Convert the image and mask to PyTorch Tensors.\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"\n",
    "    Defines the transformation pipeline for the validation dataset.\n",
    "    Only includes essential preprocessing steps, no random augmentations.\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # 1. Normalize the image to match the training data distribution.\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        # 2. Convert the image and mask to PyTorch Tensors.\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "train_trans = get_train_transforms()\n",
    "val_trans = get_val_transforms()\n",
    "\n",
    "train_trans, val_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa85bbf",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "The model uses a **UNet++** architecture with a pretrained `EfficientNet-B4` encoder, leveraging transfer learning for faster convergence and better performance. To further boost training, it incorporates **deep supervision**, which applies auxiliary loss signals to intermediate layers. During inference, only the final, full-resolution prediction is used to generate the segmentation map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561f25ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "BrainTumorSegmentationModel                                  [1, 2, 512, 512]          436\n",
       "├─UnetPlusPlus: 1-1                                          [1, 2, 512, 512]          --\n",
       "│    └─EfficientNetEncoder: 2-1                              [1, 3, 512, 512]          806,400\n",
       "│    │    └─Conv2dStaticSamePadding: 3-1                     [1, 48, 256, 256]         1,296\n",
       "│    │    └─BatchNorm2d: 3-2                                 [1, 48, 256, 256]         96\n",
       "│    │    └─SiLU: 3-3                                        [1, 48, 256, 256]         --\n",
       "│    │    └─ModuleList: 3-4                                  --                        16,740,824\n",
       "│    └─UnetPlusPlusDecoder: 2-2                              [1, 16, 512, 512]         --\n",
       "│    │    └─ModuleDict: 3-5                                  --                        3,264,352\n",
       "│    └─SegmentationHead: 2-3                                 [1, 2, 512, 512]          --\n",
       "│    │    └─Conv2d: 3-6                                      [1, 2, 512, 512]          290\n",
       "│    │    └─Identity: 3-7                                    [1, 2, 512, 512]          --\n",
       "│    │    └─Activation: 3-8                                  [1, 2, 512, 512]          --\n",
       "==============================================================================================================\n",
       "Total params: 20,813,694\n",
       "Trainable params: 20,813,694\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 31.21\n",
       "==============================================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 1314.65\n",
       "Params size (MB): 13.55\n",
       "Estimated Total Size (MB): 1331.34\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "class BrainTumorSegmentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A segmentation model using a UNet++ decoder with a pretrained EfficientNet encoder.\n",
    "    Implements deep supervision for improved training performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, encoder_name=\"efficientnet-b4\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Initialize the main model from the segmentation-models-pytorch (smp) library.\n",
    "        # UNet++ often provides better performance than standard U-Net due to its nested skip pathways.\n",
    "        self.model = smp.UnetPlusPlus(\n",
    "            encoder_name=encoder_name,      # The backbone network (e.g., 'efficientnet-b4').\n",
    "            encoder_weights=\"imagenet\",     # Use pretrained weights for transfer learning.\n",
    "            in_channels=3,                  # Expects 3-channel (RGB) input images.\n",
    "            classes=num_classes,            # Number of output classes (e.g., 2 for background/tumor).\n",
    "            activation=None,                # Output raw logits; activation is handled by the loss function.\n",
    "        )\n",
    "        \n",
    "        # Get the number of output channels from each stage of the encoder.\n",
    "        # This makes the model adaptable to different encoder backbones.\n",
    "        encoder_channels = self.model.encoder.out_channels\n",
    "        \n",
    "        # 2. Define auxiliary heads for deep supervision.\n",
    "        # This technique adds loss signals at intermediate decoder layers to improve gradient flow.\n",
    "        self.deep_supervision = True\n",
    "        if self.deep_supervision:\n",
    "            # These heads are simple 1x1 convolutions that map intermediate feature maps\n",
    "            # from the decoder to the desired number of output classes.\n",
    "            self.aux_head1 = nn.Conv2d(encoder_channels[-2], num_classes, kernel_size=1)\n",
    "            self.aux_head2 = nn.Conv2d(encoder_channels[-3], num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # --- Main Forward Pass ---\n",
    "        main_output = self.model(x)\n",
    "        \n",
    "        # --- Deep Supervision (only during training) ---\n",
    "        if self.training and self.deep_supervision:\n",
    "            # Get the feature maps from different stages of the encoder.\n",
    "            # Note: This is a simplified implementation; it re-runs the encoder.\n",
    "            features = self.model.encoder(x)\n",
    "            \n",
    "            # Create auxiliary predictions from intermediate feature maps.\n",
    "            # These features correspond to different spatial scales in the network.\n",
    "            aux1 = self.aux_head1(features[-2])  # Feature map at 1/16 resolution\n",
    "            aux2 = self.aux_head2(features[-3])  # Feature map at 1/8 resolution\n",
    "            \n",
    "            # Upsample the auxiliary predictions to match the main output's spatial dimensions.\n",
    "            aux1 = F.interpolate(aux1, size=main_output.shape[2:], mode='bilinear', align_corners=False)\n",
    "            aux2 = F.interpolate(aux2, size=main_output.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Return a dictionary of outputs. The loss function will handle combining them.\n",
    "            return {\n",
    "                'main': main_output,\n",
    "                'aux1': aux1,\n",
    "                'aux2': aux2\n",
    "            }\n",
    "        \n",
    "        # During validation or inference, only return the final, main prediction.\n",
    "        return main_output\n",
    "\n",
    "model = BrainTumorSegmentationModel()\n",
    "summary(model, input_size=(1, 3, 512, 512))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74d49b",
   "metadata": {},
   "source": [
    "## 4. ADVANCED Loss Function\n",
    "To effectively train the segmentation model, a sophisticated `CombinedLoss` function is implemented. A single loss function is often insufficient for complex medical imaging tasks, so this approach combines the strengths of three different state-of-the-art loss functions from the MONAI library:\n",
    "\n",
    "1. `DiceCELoss`: A powerful hybrid loss that combines the region-based **Dice Loss** (excellent for handling class imbalance) with the pixel-wise **Cross-Entropy Loss** (ensures good per-pixel classification). This forms the primary component of our total loss.\n",
    "\n",
    "2. `FocalLoss`: This loss modifies Cross-Entropy to down-weight the loss assigned to well-classified examples. This forces the model to focus its efforts on **hard-to-classify pixels**, such as those along the ambiguous boundaries of a tumor.\n",
    "\n",
    "3. `TverskyLoss`: A generalization of the Dice score that provides a trade-off between **false positives** and **false negatives**. In our configuration, it is tuned to penalize false positives more heavily, which can help reduce the prediction of non-tumor areas.\n",
    "\n",
    "These three losses are combined in a weighted sum. Furthermore, the `CombinedLoss` class is designed to seamlessly handle the **deep supervision** outputs from the model, applying a weighted loss to the main and auxiliary predictions to ensure robust gradient flow throughout the network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2be1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom loss function that combines multiple state-of-the-art segmentation losses.\n",
    "    Designed to handle class imbalance and focus on difficult examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.5, beta=0.3, gamma=0.2):\n",
    "        super().__init__()\n",
    "        # 1. Define weights for combining the individual loss components.\n",
    "        self.alpha = alpha  # Weight for Dice+CE loss\n",
    "        self.beta = beta    # Weight for Focal loss\n",
    "        self.gamma = gamma  # Weight for Tversky loss\n",
    "        \n",
    "        # 2. Initialize the individual loss functions from the MONAI library.\n",
    "        # Dice + CrossEntropy: A robust combination for general segmentation tasks.\n",
    "        self.dice_ce = DiceCELoss(\n",
    "            include_background=False, # Exclude background class from Dice calculation.\n",
    "            to_onehot_y=True,         # Convert target to one-hot format.\n",
    "            softmax=True,             # Apply softmax to predictions.\n",
    "            lambda_dice=0.5,          # Weight for Dice component.\n",
    "            lambda_ce=0.5,            # Weight for CrossEntropy component.\n",
    "            squared_pred=True,        # Use squared predictions in Dice denominator for smoother gradients.\n",
    "            smooth_nr=1e-5,\n",
    "            smooth_dr=1e-5\n",
    "        )\n",
    "        \n",
    "        # Focal Loss: Helps focus training on hard-to-classify pixels (e.g., boundaries).\n",
    "        self.focal = FocalLoss(\n",
    "            include_background=False,\n",
    "            to_onehot_y=True,\n",
    "            alpha=0.75,\n",
    "            gamma=2.0,                # Focusing parameter.\n",
    "            reduction='mean'          # Average the loss over the batch.\n",
    "        )\n",
    "        \n",
    "        # Tversky Loss: A generalization of Dice that allows weighting false positives and negatives.\n",
    "        self.tversky = TverskyLoss(\n",
    "            include_background=False,\n",
    "            to_onehot_y=True,\n",
    "            softmax=True,\n",
    "            alpha=0.7,  # Penalizes false positives more heavily.\n",
    "            beta=0.3,   # Penalizes false negatives less heavily.\n",
    "            smooth_nr=1e-5,\n",
    "            smooth_dr=1e-5\n",
    "        )\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # This method handles both standard and deep supervision model outputs.\n",
    "        \n",
    "        if isinstance(pred, dict):\n",
    "            # --- Handle Deep Supervision ---\n",
    "            # If the prediction is a dictionary, it contains outputs from multiple model heads.\n",
    "            main_loss = self._compute_loss(pred['main'], target)\n",
    "            aux1_loss = self._compute_loss(pred['aux1'], target)\n",
    "            aux2_loss = self._compute_loss(pred['aux2'], target)\n",
    "            \n",
    "            # Combine the losses, giving less weight to the auxiliary outputs.\n",
    "            total_loss = main_loss + 0.4 * aux1_loss + 0.2 * aux2_loss\n",
    "            return total_loss\n",
    "        \n",
    "        else:\n",
    "            # --- Handle Standard Output ---\n",
    "            # If the prediction is a single tensor, compute the loss directly.\n",
    "            return self._compute_loss(pred, target)\n",
    "    \n",
    "    def _compute_loss(self, pred, target):\n",
    "        # Helper method to calculate the weighted sum of the three loss components.\n",
    "        dice_ce_loss = self.dice_ce(pred, target)\n",
    "        focal_loss = self.focal(pred, target)\n",
    "        tversky_loss = self.tversky(pred, target)\n",
    "        \n",
    "        # Combine the individual losses using the predefined alpha, beta, and gamma weights.\n",
    "        return  (self.alpha * dice_ce_loss + \n",
    "                   self.beta * focal_loss + \n",
    "                   self.gamma * tversky_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62128484",
   "metadata": {},
   "source": [
    "## 5. Training UTILITIES\n",
    "The training process is configured using a set of modular helper functions:\n",
    "\n",
    "- `create_dataloaders`: Prepares the data by splitting it into training and validation sets, applying the correct data augmentations to each, and creating `DataLoader` instances for model consumption.\n",
    "- `setup_training`: Configures the `AdamW` optimizer and the `OneCycleLR` learning rate scheduler, which are modern, high-performance choices for training deep learning models.\n",
    "- `setup_metrics`: Initializes the key performance indicators for evaluation, including the `DiceMetric` (to measure segmentation overlap) and the `HausdorffDistanceMetric` (to measure boundary accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4877543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "def create_dataloaders(images_dir, masks_dir, batch_size=8, num_workers=0, \n",
    "                        train_split=0.8, image_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders efficiently using PyTorch Subsets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Create two dataset instances, one for training and one for validation. \n",
    "    # This still scans the directory twice but is necessary to assign different transforms.\n",
    "    train_dataset = BrainTumorDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        transform=get_train_transforms(),\n",
    "        image_size=image_size\n",
    "    )\n",
    "    \n",
    "    val_dataset = BrainTumorDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        transform=get_val_transforms(),\n",
    "        image_size=image_size\n",
    "    )\n",
    "    \n",
    "    # 2. Create a random split of indices.\n",
    "    dataset_size = len(train_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(train_split * dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "    # 3. Create PyTorch subsets with the split indices.\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "    # 4. Create dataloaders from the subsets.\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_subset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_subset)}\")\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74466aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(model, train_loader, epochs=100, max_lr=3e-4):\n",
    "    \"\"\"\n",
    "    Configures the optimizer and learning rate scheduler for model training.\n",
    "    \"\"\"\n",
    "    # 1. Initialize the AdamW optimizer.\n",
    "    # AdamW is an improved version of Adam that decouples weight decay from the gradient update,\n",
    "    # which often leads to better model generalization.\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=max_lr,           # The maximum learning rate.\n",
    "        weight_decay=1e-4,   # Regularization term to prevent overfitting.\n",
    "        betas=(0.9, 0.999),  # Coefficients for computing running averages of gradient and its square.\n",
    "        eps=1e-8\n",
    ")\n",
    "    \n",
    "    # 2. Initialize the OneCycleLR scheduler.\n",
    "    # This scheduler varies the learning rate cyclically, starting low, increasing to a\n",
    "    # maximum, and then decreasing. It's known to help models converge faster and avoid\n",
    "    # getting stuck in local minima.\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=max_lr,                      # The upper learning rate boundary.\n",
    "        epochs=epochs,                      # Total number of epochs for the cycle.\n",
    "        steps_per_epoch=len(train_loader),  # Number of batches in one epoch.\n",
    "        pct_start=0.3,                      # Percentage of the cycle spent increasing the LR.\n",
    "        div_factor=25,                      # Determines the initial LR (max_lr / div_factor).\n",
    "        final_div_factor=1e4,               # Determines the minimum LR (initial_lr / final_div_factor).\n",
    "        anneal_strategy='cos'               # Use a cosine annealing strategy for the decay phase.\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff88f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_metrics():\n",
    "    \"\"\"\n",
    "    Initializes a dictionary of metrics for evaluating segmentation performance.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        # 1. Dice Metric: Measures the overlap between prediction and ground truth.\n",
    "        # It is the primary metric for most segmentation tasks.\n",
    "        'dice': DiceMetric(\n",
    "            include_background=False, # Crucial: only evaluate the tumor class, not the background.\n",
    "            reduction=\"mean\",         # Average the score across all images in a batch.\n",
    "            get_not_nans=False\n",
    "        ),\n",
    "        \n",
    "        # 2. Hausdorff Distance: Measures the distance between the boundaries of the\n",
    "        # predicted and ground truth segmentations. Excellent for evaluating boundary accuracy.\n",
    "        'hausdorff': HausdorffDistanceMetric(\n",
    "            include_background=False, # Also focus only on the tumor class.\n",
    "            reduction=\"mean\",         # Average the score across the batch.\n",
    "            percentile=95             # Use the 95th percentile to make the metric robust to outliers.\n",
    "        )\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e7057",
   "metadata": {},
   "source": [
    "## 6. Training and Validation loops\n",
    "The training and validation loops are managed by two core functions:\n",
    "\n",
    "- `train_epoch`: Executes a single training epoch, performing forward and backward passes, calculating loss, updating model weights with `AdamW`, adjusting the learning rate via `OneCycleLR`, and utilizing Automatic Mixed Precision (AMP) for efficiency.\n",
    "- `validate_epoch`: Runs a single validation epoch, computing the loss and evaluating the model's performance using `DiceMetric` and `HausdorffDistanceMetric` without updating model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb1be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, loss_fn, scaler, device):\n",
    "    \"\"\"\n",
    "    Performs one full training epoch.\n",
    "    \"\"\"\n",
    "    # 1. Set model to training mode.\n",
    "    model.train()\n",
    "    \n",
    "    # 2. Initialize total loss for the epoch.\n",
    "    total_loss = 0 \n",
    "    \n",
    "    # 3. Setup progress bar.\n",
    "    pbar = tqdm(train_loader, desc=\"Training\") \n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # 4. Move batch data to the specified device.\n",
    "        images = batch['image'].to(device, non_blocking=True) \n",
    "        masks = batch['mask'].to(device, non_blocking=True) \n",
    "        \n",
    "        # 5. Clear previous gradients.\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # 6. Enable Automatic Mixed Precision (AMP) for efficiency.\n",
    "        with torch.amp.autocast(device_type= device):\n",
    "            # 7. Forward pass.\n",
    "            outputs = model(images) \n",
    "            # 8. Reshape mask from [B, H, W] to [B, 1, H, W] to match model output dimensions\n",
    "            #    and ensure it's a float tensor for the loss calculation.\n",
    "            masks = masks.unsqueeze(1).float()\n",
    "            loss = loss_fn(outputs, masks)             \n",
    "            \n",
    "        # 9. Scale loss and perform backward pass.\n",
    "        scaler.scale(loss).backward()\n",
    "        # 10. Update model weights.\n",
    "        scaler.step(optimizer) \n",
    "        # 11. Update the gradient scaler.\n",
    "        scaler.update() \n",
    "        \n",
    "        # 12. Update learning rate based on scheduler.\n",
    "        scheduler.step() \n",
    "        \n",
    "        # 13. Accumulate batch loss.\n",
    "        total_loss += loss.item() \n",
    "        \n",
    "        # 14. Update progress bar with current loss and learning rate.\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'LR': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    # 15. Return average epoch loss.\n",
    "    return total_loss / len(train_loader) \n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, loss_fn, metrics, device):\n",
    "    \"\"\"\n",
    "    Performs one full validation epoch.\n",
    "    \"\"\"\n",
    "    # 1. Set model to evaluation mode.\n",
    "    model.eval() \n",
    "    \n",
    "    # 2. Initialize total loss for the epoch.\n",
    "    total_loss = 0 \n",
    "    \n",
    "    # 3. Reset all evaluation metrics.\n",
    "    for metric in metrics.values():\n",
    "        metric.reset()\n",
    "    \n",
    "    # 4. Disable gradient calculations for validation.\n",
    "    with torch.inference_mode(): \n",
    "        # 5. Setup progress bar.\n",
    "        pbar = tqdm(val_loader, desc=\"Validation\") \n",
    "        \n",
    "        for batch in pbar:\n",
    "            # 6. Move batch data to the specified device.\n",
    "            images = batch['image'].to(device, non_blocking=True) \n",
    "            masks = batch['mask'].to(device, non_blocking=True) \n",
    "            \n",
    "            # 7. Enable AMP for validation (no effect on gradients).\n",
    "            with torch.amp.autocast(device_type= device): \n",
    "                # 8. Forward pass.\n",
    "                outputs = model(images) \n",
    "                # 9. Calculate loss.\n",
    "                loss = loss_fn(outputs, masks) \n",
    "            \n",
    "            # 10. Accumulate batch loss.\n",
    "            total_loss += loss.item() \n",
    "            \n",
    "            # 11. Extract main output if deep supervision is used.\n",
    "            if isinstance(outputs, dict):\n",
    "                outputs = outputs['main']\n",
    "            \n",
    "            # 12. Convert logits to probabilities.\n",
    "            preds = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            # 13. Update Dice metric.\n",
    "            metrics['dice'](preds, masks) \n",
    "            \n",
    "            # 14. Prepare binary predictions and masks for Hausdorff distance.\n",
    "            binary_preds = torch.argmax(preds, dim=1, keepdim=True) \n",
    "            binary_masks = masks.unsqueeze(1) \n",
    "            \n",
    "            # 15. Update Hausdorff metric.\n",
    "            metrics['hausdorff'](binary_preds, binary_masks) \n",
    "    \n",
    "    # 16. Aggregate and retrieve final metric scores.\n",
    "    dice_score = metrics['dice'].aggregate().item() \n",
    "    hausdorff_dist = metrics['hausdorff'].aggregate().item() \n",
    "    \n",
    "    # 17. Return average epoch loss and aggregated metrics.\n",
    "    return total_loss / len(val_loader), dice_score, hausdorff_dist \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd913f",
   "metadata": {},
   "source": [
    "## 7. MAIN TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5588b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(images_dir, masks_dir, epochs=10, batch_size=8, image_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Orchestrates the entire training process for the brain tumor segmentation model.\n",
    "    \"\"\"\n",
    "    # 1. Determine the device (GPU or CPU) for training.\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\") \n",
    "    \n",
    "    # 2. Create data loaders for training and validation sets.\n",
    "    train_loader, val_loader = create_dataloaders(images_dir=images_dir,\n",
    "                                                masks_dir=masks_dir,\n",
    "                                                batch_size=batch_size,\n",
    "                                                image_size=image_size )\n",
    "    \n",
    "    # 3. Initialize the segmentation model and move it to the device.\n",
    "    model = BrainTumorSegmentationModel(num_classes=2, \n",
    "                                        encoder_name=\"efficientnet-b4\")\n",
    "    model = model.to(device) \n",
    "    \n",
    "    # 4. Count and print model parameters for verification.\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # 5. Initialize loss function, optimizer, learning rate scheduler, metrics, and AMP scaler.\n",
    "    criterion = CombinedLoss() \n",
    "    optimizer, scheduler = setup_training(model, train_loader, epochs) \n",
    "    metrics = setup_metrics() \n",
    "    scaler = torch.amp.GradScaler(device=device) \n",
    "    \n",
    "    # 6. Initialize variables for tracking training progress.\n",
    "    best_dice = 0.0 \n",
    "    train_losses = [] \n",
    "    val_losses = [] \n",
    "    dice_scores = [] \n",
    "    \n",
    "    # 7. Main training loop.\n",
    "    for epoch in range(epochs): \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\") \n",
    "        print(\"-\" * 50) \n",
    "        \n",
    "        # 8. Execute one training epoch.\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, \n",
    "                                criterion, scaler, device)\n",
    "        \n",
    "        # 9. Execute one validation epoch.\n",
    "        val_loss, dice_score, hausdorff_dist = validate_epoch(\n",
    "            model, val_loader, criterion, metrics, device)\n",
    "        \n",
    "        # 10. Store epoch results.\n",
    "        train_losses.append(train_loss) \n",
    "        val_losses.append(val_loss) \n",
    "        dice_scores.append(dice_score) \n",
    "        \n",
    "        # 11. Print current epoch's performance.\n",
    "        print(f\"Train Loss: {train_loss:.4f}\") \n",
    "        print(f\"Val Loss: {val_loss:.4f}\") \n",
    "        print(f\"Dice Score: {dice_score:.4f}\") \n",
    "        print(f\"Hausdorff Distance: {hausdorff_dist:.4f}\") \n",
    "        \n",
    "        # 12. Check if current Dice score is the best so far.\n",
    "        if dice_score > best_dice:\n",
    "            best_dice = dice_score # 13. Update best Dice score.\n",
    "            # 14. Save model checkpoint if performance improved.\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'dice_score': dice_score,\n",
    "                'val_loss': val_loss\n",
    "            }, 'best_brain_tumor_model.pth') \n",
    "            print(f\"New best model saved! Dice: {dice_score:.4f}\") \n",
    "    \n",
    "    print(f\"\\nTraining completed! Best Dice Score: {best_dice:.4f}\") # 15. Final training summary.\n",
    "    \n",
    "    # 16. Return the trained model and historical performance data.\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'dice_scores': dice_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20648d77",
   "metadata": {},
   "source": [
    "## 8. INFERENCE AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e452ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, image_path, mask_path=None, device='cuda', threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make prediction and visualize results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (512, 512))\n",
    "    \n",
    "    # Apply same normalization as validation\n",
    "    transform = get_val_transforms()\n",
    "    transformed = transform(image=image, mask=np.zeros((512, 512)))\n",
    "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type= device):\n",
    "            output = model(input_tensor)\n",
    "            if isinstance(output, dict):\n",
    "                output = output['main']\n",
    "            \n",
    "            pred_probs = torch.softmax(output, dim=1)\n",
    "            pred_mask = (pred_probs[0, 1] > threshold).cpu().numpy()\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3 if mask_path else 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(pred_mask, cmap='jet', alpha=0.7)\n",
    "    axes[1].imshow(image, alpha=0.3)\n",
    "    axes[1].set_title('Predicted Mask')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    if mask_path:\n",
    "        true_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        true_mask = cv2.resize(true_mask, (512, 512))\n",
    "        true_mask = (true_mask > 127).astype(np.uint8)\n",
    "        \n",
    "        axes[2].imshow(true_mask, cmap='jet', alpha=0.7)\n",
    "        axes[2].imshow(image, alpha=0.3)\n",
    "        axes[2].set_title('Ground Truth Mask')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99c60f",
   "metadata": {},
   "source": [
    "## USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f8fa96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3064 valid image-mask pairs\n",
      "Found 3064 valid image-mask pairs\n",
      "Train dataset size: 2451\n",
      "Validation dataset size: 613\n"
     ]
    }
   ],
   "source": [
    "images_dir = \"brain_tumor_dataset/images\"\n",
    "masks_dir = \"brain_tumor_dataset/masks\"\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(images_dir=images_dir,\n",
    "                                                masks_dir=masks_dir,\n",
    "                                                batch_size=4,\n",
    "                                                image_size=(512, 512))\n",
    "\n",
    "train_sample = next(iter(train_loader))\n",
    "# train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b7c2a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 3064 valid image-mask pairs\n",
      "Found 3064 valid image-mask pairs\n",
      "Train dataset size: 2451\n",
      "Validation dataset size: 613\n",
      "Total parameters: 20,813,694\n",
      "Trainable parameters: 20,813,694\n",
      "\n",
      "Epoch 1/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/1225 [00:13<4:30:32, 13.26s/it, Loss=1.2126, LR=0.000012]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m masks_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrain_tumor_dataset/masks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(images_dir, masks_dir, epochs, batch_size, image_size)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m) \n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 8. Execute one training epoch.\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 9. Execute one validation epoch.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m val_loss, dice_score, hausdorff_dist \u001b[38;5;241m=\u001b[39m validate_epoch(\n\u001b[0;32m     49\u001b[0m     model, val_loader, criterion, metrics, device)\n",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, optimizer, scheduler, loss_fn, scaler, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, masks)             \n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 9. Scale loss and perform backward pass.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 10. Update model weights.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer) \n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images_dir = \"brain_tumor_dataset/images\"\n",
    "masks_dir = \"brain_tumor_dataset/masks\"\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(\n",
    "    images_dir=images_dir,\n",
    "    masks_dir=masks_dir,\n",
    "    epochs=5,\n",
    "    batch_size=2,\n",
    "    image_size=(512, 512)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
