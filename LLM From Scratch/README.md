# LLM from Scratch

This repository is a personal workspace where I study, experiment with, and implement core components of Large Language Models (LLMs), following the structure and guidance of the excellent open-source project: [rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch).

---

## About

The goal of this repo is to **learn by doing** — to gain a deeper understanding of how large-scale language models work internally, from tokenization and embeddings, to attention mechanisms and transformer blocks. All code here is built with clarity and simplicity in mind, as inspired by the original repo.

While this repo follows the concepts and structure of [rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch), it also includes my own experiments, notes, and variations as I try to internalize the ideas and push myself further.

---

## Structure

You can expect to find:

-  My own **notes and comments** to help solidify understanding.
-  **Re-implementations** of key components from scratch.
-  **Experiments** and **extensions** that go beyond the original repo.
-  Clear and clean code with focus on **readability** and **modularity**.

---

## Credits

A huge thank you to **[Sebastian Raschka](https://github.com/rasbt)** for the incredible effort behind [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch). The repo is exceptionally well-structured and accessible, and serves as both an educational resource and a technical reference. This project wouldn’t exist without it.

---

## Disclaimer

This repository is for **learning and research purposes** only. It is not intended for production use. All credit for original concepts and implementations goes to the upstream author.

---

## Progress Tracking

I'll be updating the repo as I go:

- [ ] Chapter 1
- [ ] Chapter 2
- [ ] Chapter 3
- [ ] Chapter 4
- [ ] Chapter 5
- [ ] Chapter 6
- [ ] Chapter 7

---


